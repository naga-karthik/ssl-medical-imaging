{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ssl-supervised-colab-test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/EQAFqygojxl58Q9N1v5A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naga-karthik/ssl-medical-imaging/blob/arash%2Ftest2/ssl_supervised_colab_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30An94dO8BAU"
      },
      "source": [
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "antAQJ2LOflE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fccd34a6-d56f-4b77-938d-20f4ce03c1b1"
      },
      "source": [
        "!rm -r ssl-medical-imaging\n",
        "!git clone -b arash/test2 https://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@github.com/naga-karthik/ssl-medical-imaging"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'ssl-medical-imaging': No such file or directory\n",
            "Cloning into 'ssl-medical-imaging'...\n",
            "remote: Enumerating objects: 210, done.\u001b[K\n",
            "remote: Counting objects: 100% (210/210), done.\u001b[K\n",
            "remote: Compressing objects: 100% (128/128), done.\u001b[K\n",
            "remote: Total 210 (delta 100), reused 166 (delta 74), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (210/210), 3.30 MiB | 5.47 MiB/s, done.\n",
            "Resolving deltas: 100% (100/100), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwCRcDfUaEYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b3f7d8d-6aa9-41e3-d057-ca35ddc38af0"
      },
      "source": [
        "!pip -q install -r /content/ssl-medical-imaging/requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.7 MB 7.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 523 kB 42.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 51.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 48.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 41.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 48.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 329 kB 47.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 26.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 39.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 192 kB 43.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 50.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 50.8 MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oowyq_MBTeGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f96d29-2bfa-4272-cd3d-301d02c7d09c"
      },
      "source": [
        "if not os.path.isdir('/content/ACDC'):\n",
        "  !gdown --id 1-DAdhFAG-N57YW_UZEsN2Yz2PugvgvxP\n",
        "  !unzip -q ACDC.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-DAdhFAG-N57YW_UZEsN2Yz2PugvgvxP\n",
            "To: /content/ACDC.zip\n",
            "100% 1.84G/1.84G [00:13<00:00, 134MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPQaTCD4auiy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be9eac4c-8ed8-4867-c811-88f511a858b4"
      },
      "source": [
        "if not os.path.isdir('/content/Task05_Prostate'):\n",
        "  !gdown --id 1F6zonQztBaNg8SX0rdhWdUDnH03tmuTY\n",
        "  !unzip -q ./Task05_Prostate.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1F6zonQztBaNg8SX0rdhWdUDnH03tmuTY\n",
            "To: /content/Task05_Prostate.zip\n",
            "100% 470M/470M [00:08<00:00, 56.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUp0f8H2W79U",
        "outputId": "a4627d89-c18d-43a9-e227-b73f8651701a"
      },
      "source": [
        "!python /content/ssl-medical-imaging/supervised_train.py --img_path /content/ACDC --seg_path /content/ACDC"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "{'name': 'ACDC', 'dimension': (192, 192), 'resolution': (1.367, 1.367), 'num_class': 4} final shape (487, 2, 192, 192)\n",
            "{'name': 'ACDC', 'dimension': (192, 192), 'resolution': (1.367, 1.367), 'num_class': 4} final shape (90, 2, 192, 192)\n",
            "{'name': 'ACDC', 'dimension': (192, 192), 'resolution': (1.367, 1.367), 'num_class': 4} final shape (182, 2, 192, 192)\n",
            "initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRandom-Random Strategy Run 1 <- 1638123521\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ssl-medical-imaging/supervised-train\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ssl-medical-imaging/supervised-train/runs/3bm3vji3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20211128_182009-3bm3vji3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\n",
            "  | Name | Type             | Params\n",
            "------------------------------------------\n",
            "0 | net  | SegUnetFullModel | 19.6 M\n",
            "------------------------------------------\n",
            "19.6 M    Trainable params\n",
            "0         Non-trainable params\n",
            "19.6 M    Total params\n",
            "78.321    Total estimated model params size (MB)\n",
            "Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Validation sanity check:   0% 0/2 [00:00<?, ?it/s]2\n",
            "2\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:408: UserWarning: The number of training samples (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
            "Epoch 0:   0% 0/18 [00:00<?, ?it/s] Traceback (most recent call last):\n",
            "  File \"/content/ssl-medical-imaging/supervised_train.py\", line 165, in <module>\n",
            "    main(cfg)\n",
            "  File \"/content/ssl-medical-imaging/supervised_train.py\", line 162, in main\n",
            "    trainer.fit(model)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 738, in fit\n",
            "    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 682, in _call_and_handle_interrupt\n",
            "    return trainer_fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 772, in _fit_impl\n",
            "    self._run(model, ckpt_path=ckpt_path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1195, in _run\n",
            "    self._dispatch()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1274, in _dispatch\n",
            "    self.training_type_plugin.start_training(self)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 202, in start_training\n",
            "    self._results = trainer.run_stage()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1284, in run_stage\n",
            "    return self._run_train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1314, in _run_train\n",
            "    self.fit_loop.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 145, in run\n",
            "    self.advance(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 234, in advance\n",
            "    self.epoch_loop.run(data_fetcher)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 145, in run\n",
            "    self.advance(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 193, in advance\n",
            "    batch_output = self.batch_loop.run(batch, batch_idx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 145, in run\n",
            "    self.advance(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 88, in advance\n",
            "    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 145, in run\n",
            "    self.advance(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 219, in advance\n",
            "    self.optimizer_idx,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 266, in _run_optimization\n",
            "    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 386, in _optimizer_step\n",
            "    using_lbfgs=is_lbfgs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py\", line 1664, in optimizer_step\n",
            "    optimizer.step(closure=optimizer_closure)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/optimizer.py\", line 164, in step\n",
            "    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\", line 336, in optimizer_step\n",
            "    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 163, in optimizer_step\n",
            "    optimizer.step(closure=closure, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\", line 65, in wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/adamw.py\", line 92, in step\n",
            "    loss = closure()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 148, in _wrap_closure\n",
            "    closure_result = closure()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 160, in __call__\n",
            "    self._result = self.closure(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 155, in closure\n",
            "    self._backward_fn(step_output.closure_loss)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 327, in backward_fn\n",
            "    self.trainer.accelerator.backward(loss, optimizer, opt_idx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\", line 311, in backward\n",
            "    self.precision_plugin.backward(self.lightning_module, closure_loss, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 91, in backward\n",
            "    model.backward(closure_loss, optimizer, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py\", line 1446, in backward\n",
            "    loss.backward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 307, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 156, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [32, 128, 6, 6]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 250... (failed 1). Press ctrl-c to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              lr-AdamW ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              lr-AdamW 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mRandom-Random Strategy Run 1 <- 1638123521\u001b[0m: \u001b[34mhttps://wandb.ai/ssl-medical-imaging/supervised-train/runs/3bm3vji3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211128_182009-3bm3vji3/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2BIxvD5xTvC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}